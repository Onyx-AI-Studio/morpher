{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "81cb91f8-84d1-4592-9e27-f41e9f3bb955",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "!pip install cohere langchain -qqq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "112d4aeb-8270-417b-80b6-88c9dbcf1c48",
   "metadata": {},
   "outputs": [
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      " ········\n"
     ]
    }
   ],
   "source": [
    "from getpass import getpass\n",
    "\n",
    "COHERE_API_KEY = getpass()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6cdbf73f-a741-4f72-bae0-7b44d85094fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.llms import Cohere\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain.chains import LLMChain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "b49b59aa-b3e8-430f-a858-bbf686fa8c5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "template = \"\"\"Question: {question}\n",
    "\n",
    "Answer: Let's think step by step.\n",
    "\"\"\"\n",
    "\n",
    "prompt = PromptTemplate(template=template, input_variables=[\"question\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "b55fc5d8-f27d-4f77-9931-0fa433b55dc6",
   "metadata": {},
   "outputs": [],
   "source": [
    "llm = Cohere(cohere_api_key=COHERE_API_KEY, max_tokens=8192)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "4acdd3d1-592a-466d-894f-eb26774c9d64",
   "metadata": {},
   "outputs": [],
   "source": [
    "llm_chain = LLMChain(prompt=prompt, llm=llm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "c6bacf67-e61b-4859-bda8-3e5356677f68",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Justin Bieber was born on March 1, 1994.\n",
      "\n",
      "Therefore we are looking for the NFL team that won the Super Bowl of the year 1994.\n",
      "\n",
      "In 1994, Super Bowl XXVIII was played on January 30, 1994 and featured the Dallas Cowboys beat the Buffalo Bills 30-13.\n",
      "\n",
      "So the answer is the Dallas Cowboys.\n"
     ]
    }
   ],
   "source": [
    "question = \"What NFL team won the Super Bowl in the year Justin Beiber was born?\"\n",
    "\n",
    "print(llm_chain.run(question))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "9c97d49e-e3ab-4a64-95cd-d49e9f6bf131",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cohere and Google are companies that have created models to help their artificial intelligence (AI) systems better understand language. These models are like virtual brains that let the AI systems comprehend words and sentences like humans do.\n",
      "\n",
      "Cohere's command model is like a teacher telling a class of students to do certain things. It provides clear and detailed instructions to the AI system, which helps it understand and generate language in a more structured way. The command model breaks down language into different parts, like words and sentences, and tells the AI system how to put them together to create meaningful communication.\n",
      "\n",
      "On the other hand, Google's PALM model is like a clever student who learns language all on its own, through watching and listening. While the model doesn't receive direct instructions like the command model, it's trained on a massive amount of text and learns to predict what word or sentence comes next all by itself. The PALM model thinks about language in a more holistic way, considering the context and sentiment behind the words to generate more natural-sounding responses.\n",
      "\n",
      "In summary, while the Cohere command model uses instruction to directly guide its language understanding and generation, Google's PALM model learns indirectly through self-training, aiming to produce more human-like language capabilities.\n"
     ]
    }
   ],
   "source": [
    "question = \"Explain like I'm 5, What is the difference between Cohere's command model and Google's PALM model?\"\n",
    "\n",
    "print(llm_chain.run(question).strip())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "74307b96-003d-4b01-9001-4af176aa60c7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Imagine you have a special notebook where you can write instructions for solving different problems or answering questions. This notebook is very smart and can understand and follow your instructions exactly. It's like having your own personal assistant who always listens to you and does whatever you ask.\n",
      "\n",
      "Now, Cohere's command model is like a very advanced and powerful version of this imaginary notebook. It's a software system that can understand and execute commands you give it, just like a robot or a smart assistant. The model is trained on vast amounts of data and learns to understand the patterns and relationships in the data.\n",
      "\n",
      "Here's how it works internally:\n",
      "\n",
      "1. **Input**: You give the model a question or a task, just like you would give instructions to a person. The model takes your input, which can be in the form of text, like a sentence or a paragraph.\n",
      "\n",
      "2. **Understanding**: Just like a person, the model breaks down your input into smaller parts to understand it better. It analyzes the words, their meanings, and how they relate to each other to grasp the context of your command.\n",
      "\n",
      "3. **Execution**: Once the model understands your input, it uses its knowledge and the patterns it learned from the training data to execute the\n"
     ]
    }
   ],
   "source": [
    "question = \"Explain like I'm 5, tell me about Cohere's command model? How does it work internally?\"\n",
    "\n",
    "print(llm_chain.run(question).strip())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "b9facd1d-ade1-4694-9021-4368778cf258",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "When you're looking for information online, search engines like Cohere's reranker, Elasticsearch, and Solr help you find the best results from websites and other documents. They do this by looking at the words in your query and comparing them to the words in the documents. However, just comparing words isn't always enough to get the best results. That's where re-ranking comes in.\n",
      "\n",
      "reranker: Think of it like this. You have a pile of blocks (documents), and you're looking for a specific type of block that matches your search (query). A reranker is like having a superpower that lets you quickly pick out the blocks that are most similar to what you're looking for. It does this by looking at many features of the blocks, like their colors (words), shapes (how the words are organized), and sizes (how important certain words are). The reranker puts the best matching blocks on the top, so you can easily find what you're looking for.\n",
      "\n",
      "Elasticsearch and Solr: These are also search engines, but they work a bit differently from Cohere's reranker. They start by finding all the blocks (documents) that have any similarity to your search (query). Then,\n"
     ]
    }
   ],
   "source": [
    "question = \"Explain like I'm 5, What's the difference between Cohere's reranker and elasticsearch or solr dense vector re-ranking?\"\n",
    "\n",
    "print(llm_chain.run(question).strip())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "cb1e886d-1b26-4cc3-b456-7ba15edef517",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The Cohere trial key allows users to try out Cohere's API for a limited period of time, usually for evaluating or testing purposes. On the other hand, the Cohere prod key refers to the key that grants access to the production environment of Cohere's API, which is the version of the API used for real-world applications and actual usage.\n",
      "\n",
      "The Cohere trial key may have certain usage limits imposed to restrict the scope of testing and prevent abuse. These limits could include restrictions on the number of requests per month, the number of API calls within a given timeframe, or the volume of data processed. These limitations allow Cohere to manage resources effectively and prevent excessive usage during the trial period.\n",
      "\n",
      "Once a user decides to transition from testing to using Cohere's API for their application in production, they would typically obtain a Cohere prod key. The Cohere prod key removes any usage limits imposed by the trial key and allows the user to access the API for their intended application in a live production environment.\n",
      "\n",
      "In summary, the Cohere trial key is for testing and evaluation purposes with possible usage limits, while the Cohere prod key grants access to the production environment without any limitations for real-world application usage.\n",
      "\n",
      "Would you like me\n"
     ]
    }
   ],
   "source": [
    "question = \"What is the difference between a Cohere trail key and a Cohere prod key? Are there any limits on the trail key?\"\n",
    "\n",
    "print(llm_chain.run(question).strip())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2ccc4be-b8c1-41f8-bf8e-f6f01aef13e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "question = \"Is Cohere's command model suitable for building autonomous agents?\"\n",
    "\n",
    "print(llm_chain.run(question).strip())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9fab0139-cd4b-46bb-8b90-15e9e9be45a2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d615e24-b6f2-4e70-8197-809b19e8c292",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a646812-adef-4d00-9395-514a1c224984",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4889ccf-5d5e-47dc-9ea9-f9e1aa22eac9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d88c2cc4-6fe1-46e8-8c50-742eb084973f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a8e9f32-ada5-448e-93db-ed52471e5ec2",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
